# Hybrid FastAPI + FastMCP + Event Sourcing - DETAILED Implementation Checklist

**CRITICAL FINDINGS from Codebase Analysis**:
- âš ï¸ Existing event system in `src/orchestrator/events.py` (simple, no vector clocks)
- âš ï¸ Streaming already exists in `src/orchestrator/streaming.py` (AsyncIterator pattern)
- âš ï¸ Global state dictionaries NOT thread-safe (StateManager, InterventionManager)
- âš ï¸ AsyncLogger file writes NOT atomic (race conditions)
- âš ï¸ TestRunner uses subprocess.run (blocks event loop, not truly async)

---

## Phase 1: Event Store Foundation ğŸ—ï¸

### 1.1 Core Event Types

**File**: `src/events/__init__.py` (â‰¤20 lines)
- [ ] Create module structure
- [ ] Export: Event, EventType, EventStore, VectorClock, StateProjection, EventOrdering

**File**: `src/events/event_types.py` (â‰¤100 lines)
- [ ] Define `EventType` enum with 18 types

**INTEGRATION NOTE**: Must merge with existing `src/orchestrator/events.py`:
- Existing: TASK_STARTED, PLANNING_COMPLETE, MODULE_STARTED, MODULE_ITERATION, TASK_COMPLETE
- New additions: CODE_GENERATED, CORRECTION_STARTED, CORRECTION_COMPLETED, BUG_REPORT_RECEIVED, TEST_STARTED, TEST_PASSED, TEST_FAILED

**File**: `src/events/event.py` (â‰¤100 lines)
- [ ] Define `Event` dataclass (frozen=True)
- [ ] Add `__post_init__` validation
- [ ] Add `to_dict()` method

**PASSING CRITERIA**:
- âœ… Event is immutable (frozen=True enforced)
- âœ… event_id is valid UUID format
- âœ… task_id is non-empty string
- âœ… timestamp is valid ISO 8601
- âœ… sequence_number is positive integer
- âœ… code_version â‰¥ 0

**FAILING CRITERIA**:
- âŒ Event modified after creation (raises FrozenInstanceError)
- âŒ event_id is None or empty
- âŒ task_id is None, empty, or non-string
- âŒ vector_clock is not Dict[str, int]
- âŒ data contains non-serializable objects

**EDGE CASES**:
1. Empty vector_clock {} - should be valid
2. causation_id = None - valid for root events
3. data = {} - valid empty metadata
4. Large data dict (>1MB) - should handle
5. Unicode in task_id - should sanitize
6. Negative code_version - should reject
7. Future timestamp - should reject
8. sequence_number = 0 - should allow (initial event)

---

### 1.2 Vector Clock Implementation

**File**: `src/events/vector_clock.py` (â‰¤100 lines)
- [ ] Implement `VectorClock` class

**PASSING CRITERIA**:
- âœ… tick() increments process_id counter by exactly 1
- âœ… merge() takes max of each process value
- âœ… happens_before() correctly identifies causal precedence
- âœ… concurrent() returns True when no causal relationship exists
- âœ… Empty clocks handled (return False for happens_before)

**FAILING CRITERIA**:
- âŒ tick() increments by â‰ 1
- âŒ merge() doesn't take max (takes min, sum, etc.)
- âŒ happens_before() false positive (clock1 || clock2 returns True)
- âŒ happens_before() false negative (clock1 < clock2 returns False)
- âŒ concurrent() returns True for causal events

**EDGE CASES**:
1. **Empty clocks**: {} vs {} - should be concurrent
2. **Single process**: {"p1": 5} vs {"p1": 3} - should be happens_before
3. **Disjoint processes**: {"p1": 1} vs {"p2": 1} - should be concurrent
4. **Subset relation**: {"p1": 1} vs {"p1": 1, "p2": 1} - happens_before
5. **Equal clocks**: {"p1": 5, "p2": 3} vs {"p1": 5, "p2": 3} - concurrent (not <)
6. **Negative values**: {"p1": -1} - should reject or sanitize
7. **Process ID collision**: Same process_id from different sources
8. **Very large values**: {"p1": 2^63} - integer overflow check

---

### 1.3 Event Store

**File**: `src/events/event_store.py` (â‰¤100 lines)
- [ ] Implement `EventStore` class

**PASSING CRITERIA**:
- âœ… append() is thread-safe (asyncio.Lock)
- âœ… Sequence numbers increment by exactly 1 with no gaps
- âœ… Persistence succeeds and file is readable
- âœ… get_events() filters by task_id correctly
- âœ… get_events(after_sequence=N) returns only events > N
- âœ… Concurrent appends don't corrupt sequence

**FAILING CRITERIA**:
- âŒ Sequence number gaps exist (e.g., 1, 2, 4...)
- âŒ Sequence numbers repeat (e.g., 1, 2, 2, 3)
- âŒ Concurrent appends cause race condition
- âŒ Persistence fails silently
- âŒ get_events() returns events from wrong task
- âŒ File corruption on interrupted write

**EDGE CASES**:
1. **Concurrent append from 10 tasks** - all should get unique sequences
2. **Disk full during persist** - should handle gracefully
3. **File permissions** - should handle permission errors
4. **Large event data** (10MB) - should handle or reject with clear error
5. **Very long task_id** (1000 chars) - should handle or reject
6. **Special chars in task_id** (`/`, `\`, null) - should sanitize
7. **append() interrupted mid-write** - should not corrupt store
8. **get_events() with negative after_sequence** - should handle
9. **Empty store** - get_events() should return []
10. **Million events** - performance should be acceptable (<1s)

---

### 1.4 State Projections

**File**: `src/events/projections.py` (â‰¤100 lines)
- [ ] Implement `StateProjection` class

**PASSING CRITERIA**:
- âœ… rebuild_state() replays all events in order
- âœ… State matches last code_version from events
- âœ… Stale events (old version) are ignored
- âœ… All EventType transitions handled
- âœ… No events = empty state (not error)

**FAILING CRITERIA**:
- âŒ rebuild_state() misses events
- âŒ rebuild_state() replays in wrong order
- âŒ State includes data from stale events
- âŒ Unknown EventType causes crash
- âŒ Empty event list causes error

**EDGE CASES**:
1. **No events for task_id** - should return empty initialized state
2. **Out-of-order events** (by timestamp) - should use sequence_number
3. **Duplicate sequence numbers** - should detect and error
4. **Missing causation chain** - should handle gracefully
5. **BUG_REPORT for old version** - should ignore if code_version doesn't match
6. **CORRECTION_COMPLETED without CORRECTION_STARTED** - should handle
7. **Multiple CODE_GENERATED** - should keep latest version
8. **Event with invalid data structure** - should handle or validate
9. **Very long event chain** (10k events) - performance check (<5s)
10. **CODE_GENERATED with version < current** - should reject as stale

---

### 1.5 Event Ordering Utilities

**File**: `src/events/ordering.py` (â‰¤100 lines)
- [ ] Implement `EventOrdering` class

**PASSING CRITERIA**:
- âœ… is_report_valid_for_current_code() uses all 3 methods (version, clock, hash)
- âœ… Version mismatch â†’ False
- âœ… Clock causality (report before correction) â†’ False
- âœ… Hash mismatch â†’ False
- âœ… All checks pass â†’ True

**FAILING CRITERIA**:
- âŒ Only checks 1 or 2 of the 3 methods
- âŒ Version match + clock mismatch â†’ returns True (should be False)
- âŒ Hash missing in event â†’ crashes (should handle gracefully)

**EDGE CASES**:
1. **Report and correction same version, concurrent clocks** - should use hash
2. **Hash missing in both events** - should fall back to version+clock
3. **code_hash is None** - should handle gracefully
4. **Report version > correction version** - invalid state, should detect
5. **Correction without hash** - should handle
6. **Empty causation chain** - should handle
7. **Report with future timestamp** - should not affect ordering (use sequence)

### ğŸ§ª TEST GATE 1: Event Store Tests

**File**: `tests/test_event_store.py` (â‰¤100 lines)

```python
# Test 1: Sequential append
âœ… PASS: Events [1,2,3,4,5] have sequence_numbers [1,2,3,4,5]
âŒ FAIL: Any gap, duplicate, or out-of-order sequence

# Test 2: Concurrent append
âœ… PASS: 100 concurrent appends produce 100 unique sequences [1..100]
âŒ FAIL: Any collision, gap, or duplicate

# Test 3: Persistence
âœ… PASS: After append, file exists and is valid JSON
âœ… PASS: Load from file reproduces exact events
âŒ FAIL: File corrupt, missing, or events lost

# Test 4: Filtering
âœ… PASS: get_events("task-A") returns only task-A events
âœ… PASS: get_events("task-B", after_sequence=5) returns only seq > 5
âŒ FAIL: Wrong events returned, or missing events

# Test 5: Thread safety
âœ… PASS: 10 async tasks appending 10 events each = 100 total, all unique
âŒ FAIL: Race condition, lost events, or duplicate sequences
```

**File**: `tests/test_vector_clock.py` (â‰¤100 lines)

```python
# Test 1: Causality detection
âœ… PASS: {"p1":1} happens_before {"p1":2} â†’ True
âœ… PASS: {"p1":1} happens_before {"p1":1,"p2":1} â†’ True
âŒ FAIL: Returns False for clear causal relationship

# Test 2: Concurrency detection
âœ… PASS: {"p1":1} concurrent {"p2":1} â†’ True
âœ… PASS: {"p1":2,"p2":1} concurrent {"p1":1,"p2":2} â†’ True
âŒ FAIL: Returns False for clearly concurrent events

# Test 3: Merge correctness
âœ… PASS: merge({"p1":5},{"p1":3,"p2":1}) â†’ {"p1":5,"p2":1}
âŒ FAIL: Doesn't take max, or loses processes

# Test 4: Edge cases
âœ… PASS: {} concurrent {} â†’ True
âœ… PASS: {"p1":0} handled correctly
âŒ FAIL: Crash on empty clocks or zero values
```

**File**: `tests/test_projections.py` (â‰¤100 lines)

```python
# Test 1: State rebuild
âœ… PASS: Events [CODE_GEN(v1), CORRECTION(v2)] â†’ state.code_version=2
âŒ FAIL: Wrong version, or missing updates

# Test 2: Stale event filtering
âœ… PASS: BUG_REPORT(v1) after CORRECTION(v2) â†’ ignored in state
âŒ FAIL: Stale data included in state

# Test 3: All event type handling
âœ… PASS: Each of 18 EventTypes processes without error
âŒ FAIL: Unknown EventType causes crash

# Test 4: Empty events
âœ… PASS: rebuild_state([]) â†’ empty initialized state, no error
âŒ FAIL: Crashes or returns None
```

**Run Tests**:
```bash
pytest tests/test_event_store.py tests/test_vector_clock.py tests/test_projections.py -v --tb=short
```

â›” **STOP HERE IF ANY TEST FAILS** â›”

**Debug checklist if tests fail**:
- [ ] Check asyncio.Lock is acquired before all dict/list modifications
- [ ] Check sequence_number assignment happens inside lock
- [ ] Check file writes are atomic (write to temp, then rename)
- [ ] Check vector clock merge uses `max()` not `min()`
- [ ] Check happens_before uses `all()` and `any()` correctly

---

## Phase 2: Task Orchestration ğŸ¯

### 2.1 Priority System

**File**: `src/orchestrator/priority.py` (â‰¤100 lines)
- [ ] Define `TaskPriority` IntEnum
- [ ] Implement `TaskPriorityAssigner`

**PASSING CRITERIA**:
- âœ… Priority values are integers (0, 10, 20, 30, 40)
- âœ… CRITICAL < HIGH < MEDIUM < LOW < BACKGROUND
- âœ… assign_priority() maps event types correctly
- âœ… should_preempt() returns True only if â‰¥20 priority difference

**FAILING CRITERIA**:
- âŒ Priority comparison broken (HIGH > CRITICAL)
- âŒ assign_priority() returns wrong priority for event
- âŒ should_preempt() allows 10-point difference (should be 20)

**EDGE CASES**:
1. **Unknown EventType** - should default to BACKGROUND
2. **None event** - should handle gracefully
3. **Event without event_type field** - should handle
4. **Negative priority** - should reject
5. **Priority > 100** - should handle or reject

---

### 2.2 Task Orchestrator

**File**: `src/orchestrator/task_orchestrator.py` (â‰¤100 lines)
- [ ] Implement `TaskOrchestrator` class

**PASSING CRITERIA**:
- âœ… Queue accepts tasks until maxsize (100) reached
- âœ… 101st task blocks until queue has space
- âœ… Semaphore limits to max_concurrent (5) simultaneous tasks
- âœ… Priority ordering: CRITICAL tasks execute before BACKGROUND
- âœ… Workers process tasks from queue continuously
- âœ… Timeout cancels long-running tasks
- âœ… shutdown() cancels all pending and active tasks

**FAILING CRITERIA**:
- âŒ Queue accepts 101+ tasks without blocking
- âŒ More than max_concurrent tasks run simultaneously
- âŒ Lower priority task executes before higher priority
- âŒ Workers stop processing queue
- âŒ Timeout doesn't cancel task
- âŒ shutdown() leaves tasks running

**EDGE CASES**:
1. **submit_task() with queue full** - should block until space available
2. **Worker crash** - should restart worker or handle gracefully
3. **Task raises exception** - should log and continue, not crash worker
4. **Task with timeout=0** - should cancel immediately
5. **submit_task() during shutdown** - should reject new tasks
6. **Circular task dependencies** - should detect or handle
7. **Task that never yields** - should timeout
8. **1000 tasks submitted rapidly** - should handle (100 queued, rest block)
9. **get_queue_stats() during high load** - should not block
10. **Duplicate task_id** - should handle or reject

**CRITICAL INTEGRATION**: Must handle existing global StateManager without race conditions!

---

### 2.3 Backpressure Monitoring

**File**: `src/orchestrator/backpressure.py` (â‰¤100 lines)
- [ ] Implement `BackpressureMonitor` class

**PASSING CRITERIA**:
- âœ… Alerts when queue is 80% full
- âœ… CRITICAL alert when queue is 100% full
- âœ… No false positives (alerting when queue <80%)

**FAILING CRITERIA**:
- âŒ No alert when queue 90% full
- âŒ Alert spam (multiple alerts per second)
- âŒ Crash when queue stats unavailable

**EDGE CASES**:
1. **Queue oscillating at 79-81%** - should debounce alerts
2. **Rapid queue drain** - should clear alert quickly
3. **Multiple orchestrators** - each should monitor independently

### ğŸ§ª TEST GATE 2: Task Orchestration Tests

**File**: `tests/test_task_orchestrator.py` (â‰¤100 lines)

```python
# Test 1: Priority ordering
âœ… PASS: Submit [LOW, CRITICAL, HIGH] â†’ executes in order [CRITICAL, HIGH, LOW]
âŒ FAIL: Wrong execution order

# Test 2: Max concurrent limit
âœ… PASS: Start 10 long tasks â†’ max 5 run simultaneously
âŒ FAIL: 6+ tasks run simultaneously

# Test 3: Backpressure (bounded queue)
âœ… PASS: Submit 105 tasks â†’ first 100 queue, last 5 block
âœ… PASS: After 1 completes, blocked task enters queue
âŒ FAIL: All 105 tasks enter queue immediately

# Test 4: Worker pool
âœ… PASS: 3 workers continuously process queue
âœ… PASS: 1 worker crash â†’ other 2 continue
âŒ FAIL: Workers stop processing

# Test 5: Timeout enforcement
âœ… PASS: Task with timeout=1s running 2s â†’ cancelled at 1s
âŒ FAIL: Task runs beyond timeout

# Test 6: Graceful shutdown
âœ… PASS: shutdown() cancels 20 queued + 5 active tasks
âœ… PASS: All tasks receive CancelledError
âŒ FAIL: Tasks left running after shutdown
```

**File**: `tests/test_priority_assignment.py` (â‰¤100 lines)

```python
# Test 1: Priority mapping
âœ… PASS: USER_INTERVENTION â†’ CRITICAL (0)
âœ… PASS: CODE_GENERATION â†’ HIGH (10)
âœ… PASS: DEAD_CODE_ANALYSIS â†’ LOW (30)
âŒ FAIL: Wrong priority assigned

# Test 2: Preemption logic
âœ… PASS: CRITICAL (0) should_preempt MEDIUM (20) â†’ True
âœ… PASS: HIGH (10) should_preempt MEDIUM (20) â†’ False (only 10 diff)
âŒ FAIL: Preemption logic incorrect
```

**Run Tests**:
```bash
pytest tests/test_task_orchestrator.py tests/test_priority_assignment.py -v -s
```

â›” **STOP HERE IF ANY TEST FAILS** â›”

**Debug checklist**:
- [ ] Check PriorityQueue correctly orders by priority (lower = higher)
- [ ] Check Semaphore(5) enforces max concurrent
- [ ] Check asyncio.Queue(maxsize=100) blocks on 101st put()
- [ ] Check workers use `while True` loop
- [ ] Check timeout uses asyncio.wait_for()

---

## Phase 3: Context Management for Small LLMs ğŸ§ 

### 3.1 Context Manager

**File**: `src/context/context_manager.py` (â‰¤100 lines)
- [ ] Implement `ContextManager` class

**PASSING CRITERIA**:
- âœ… Token estimate within 20% of actual (chars/4 Â± 20%)
- âœ… Routes to larger model when >80% of context limit
- âœ… 4-layer context structure maintained
- âœ… Cached prompts reused (not regenerated)

**FAILING CRITERIA**:
- âŒ Token estimate off by >30%
- âŒ Context exceeds model limit (causes LLM error)
- âŒ Routing to larger model fails
- âŒ Cache misses when it should hit

**EDGE CASES**:
1. **Event with 50K chars** - should trigger model routing
2. **mistral:7b at 7.5K tokens** - should route to qwen3:8b
3. **qwen3:8b at 30K tokens** - should route to claude (or reject)
4. **Unicode/emoji heavy text** - token estimate should handle
5. **Empty event history** - should still build valid context
6. **Very long system prompt** (5K tokens) - should include in estimate
7. **Cache warm, then clear** - should regenerate correctly

---

### 3.2 Event Summarizer

**File**: `src/context/event_summarizer.py` (â‰¤100 lines)
- [ ] Implement `EventSummarizer` class

**PASSING CRITERIA**:
- âœ… Compression achieves â‰¥5x token reduction
- âœ… Summary includes key statistics (module count, iteration count)
- âœ… Recent events (last 5) kept in full detail
- âœ… Older events compressed to 1-line summaries

**FAILING CRITERIA**:
- âŒ Compression <3x (not effective enough)
- âŒ Summary missing critical info (current version, status)
- âŒ Recent events lost or corrupted

**EDGE CASES**:
1. **keep_recent=5 but only 3 events** - should keep all 3
2. **1000 old events** - compression should still be fast (<100ms)
3. **Events with large data fields** - should truncate data in compression
4. **No events** - should return minimal summary, not error

---

### 3.3 System Prompt Cache

**File**: `src/context/prompt_cache.py` (â‰¤100 lines)
- [ ] Implement `SystemPromptCache` class

**PASSING CRITERIA**:
- âœ… First call generates prompt (cache miss)
- âœ… Second call reuses prompt (cache hit)
- âœ… clear_cache() invalidates all entries
- âœ… Different event_types have different prompts

**FAILING CRITERIA**:
- âŒ Cache always misses (regenerates every time)
- âŒ Cache returns wrong prompt for event_type
- âŒ clear_cache() doesn't actually clear

**EDGE CASES**:
1. **Unknown event_type** - should generate generic prompt
2. **Cache with 100 entries** - should handle (no size limit)
3. **Prompt generation fails** - should retry or use fallback

---

### 3.4 Relevance Filter

**File**: `src/context/relevance_filter.py` (â‰¤100 lines)
- [ ] Implement `RelevanceFilter` class

**PASSING CRITERIA**:
- âœ… Returns â‰¤limit events
- âœ… Events in chronological order
- âœ… Filters by: same module, same version, causal chain
- âœ… Empty result if no relevant events

**FAILING CRITERIA**:
- âŒ Returns >limit events
- âŒ Events out of chronological order
- âŒ Includes irrelevant events (wrong module/version)

**EDGE CASES**:
1. **limit=10 but only 3 relevant events** - return 3
2. **All events irrelevant** - return []
3. **Causation chain broken** - should still return what's available
4. **Events from multiple tasks mixed** - should filter correctly

### ğŸ§ª TEST GATE 3: Context Management Tests

**File**: `tests/test_context_manager.py` (â‰¤100 lines)

```python
# Test 1: 4-layer context
âœ… PASS: Context has [system_prompt, state_summary, compressed_events, current_event]
âŒ FAIL: Missing layer or wrong order

# Test 2: Token estimation
âœ… PASS: 10K chars â†’ ~2500 tokens estimate (Â±20%)
âŒ FAIL: Estimate off by >30%

# Test 3: Model routing
âœ… PASS: mistral:7b at 7K tokens â†’ routes to qwen3:8b
âœ… PASS: qwen3:8b at 28K tokens â†’ routes to larger model
âŒ FAIL: Doesn't route, or routes at wrong threshold

# Test 4: Cache effectiveness
âœ… PASS: Same event_type called twice â†’ prompt identical, not regenerated
âŒ FAIL: Cache miss on second call
```

**File**: `tests/test_event_summarizer.py` (â‰¤100 lines)

```python
# Test 1: Compression ratio
âœ… PASS: 50 events (10K tokens) â†’ summary (1.5K tokens) = 6.6x compression
âŒ FAIL: Compression <3x

# Test 2: keep_recent
âœ… PASS: 20 events, keep_recent=5 â†’ 5 full + 15 compressed
âŒ FAIL: Wrong number kept full, or order wrong

# Test 3: Summary content
âœ… PASS: Summary includes module count, iteration count, current version
âŒ FAIL: Missing critical statistics
```

**File**: `tests/test_relevance_filter.py` (â‰¤100 lines)

```python
# Test 1: Limit enforcement
âœ… PASS: 100 events, limit=10 â†’ returns exactly 10
âŒ FAIL: Returns â‰ 10

# Test 2: Relevance filtering
âœ… PASS: 50 events, same module â†’ returns only same-module events
âŒ FAIL: Returns events from other modules

# Test 3: Chronological order
âœ… PASS: Events returned in sequence_number order
âŒ FAIL: Out of order
```

**Run Tests**:
```bash
pytest tests/test_context_manager.py tests/test_event_summarizer.py tests/test_relevance_filter.py -v
```

â›” **STOP HERE IF ANY TEST FAILS** â›”

---
# Phase 3 Enhancements: Networked Cache for Small LLMs

**CRITICAL RESEARCH FINDINGS**:
- âš ï¸ Simple compression loses 3rd/4th degree relationships
- âš ï¸ Temporal causality lost in flat summaries
- âš ï¸ Small LLMs (7B-8B) need aggressive token management
- âš ï¸ Graph-based cache with explicit references preserves 70% information vs 50% in flat summaries
- âš ï¸ Event sourcing enables cache reconstruction and debugging

**CONTEXT**: This enhanced Phase 3 addresses the critical issue where context compression wipes out event details, causing small LLMs to forget "what, why, how" of tasks. We implement a **networked cache** with explicit references, leveraging event sourcing for reconstruction.

---

### 3.5 Networked Cache (Online + Offline)

**File**: `src/context/networked_cache.py` (â‰¤100 lines)
- [ ] Implement `CacheNode` dataclass
- [ ] Implement `NetworkedCache` class with graph structure

**PASSING CRITERIA**:
- âœ… Each node has explicit references to dependencies and related nodes
- âœ… Graph traversal retrieves related context within 2 hops
- âœ… get_related_context() respects token budget
- âœ… Nodes stored in NetworkX DiGraph for relationship queries

**FAILING CRITERIA**:
- âŒ Nodes lack references (isolated summaries)
- âŒ Graph traversal doesn't respect token limit
- âŒ Related context retrieval fails or times out

**EDGE CASES**:
1. **Circular dependencies** (Aâ†’Bâ†’Câ†’A) - should detect and handle
2. **Orphaned nodes** (no dependencies/references) - should still store
3. **Deep dependency chain** (10 levels) - should limit depth to 3
4. **Large graph** (1000+ nodes) - should use efficient graph algorithms
5. **Missing referenced node** - should handle gracefully

**Architecture**:
```python
@dataclass
class CacheNode:
    id: str  # "module:validators.py", "func:validate_email"
    type: str  # "task", "module", "function", "test", "concept"
    summary: str  # Compressed summary (100-500 tokens)
    full_content_ref: str  # Reference to full content in EventStore
    dependencies: List[str]  # IDs of nodes this depends on
    references: List[str]  # IDs this node mentions
    vector_clock: Dict[str, int]  # For cache invalidation
    metadata: Dict[str, Any]  # version, timestamp, etc.
```

**Graph Structure**:
```
[Task: task-123]
â”œâ”€ contains â†’ [Module: validators.py]
â”œâ”€ contains â†’ [Module: test_validators.py]

[Module: validators.py]
â”œâ”€ depends_on â†’ [Library: pydantic]
â”œâ”€ depends_on â†’ [Library: re]
â”œâ”€ contains â†’ [Function: validate_email]
â”œâ”€ contains â†’ [Function: validate_phone]
â”œâ”€ tested_by â†’ [Module: test_validators.py]

[Function: validate_email]
â”œâ”€ depends_on â†’ [Library: re]
â”œâ”€ tested_by â†’ [Test: test_email_validation]
â”œâ”€ references â†’ [Concept: RFC 5322]
```

---

### 3.6 Online Cache (Live Context During Execution)

**File**: `src/context/online_cache.py` (â‰¤100 lines)
- [ ] Implement `OnlineCache` class
- [ ] Implement priority-based eviction (LRU + priority)

**PASSING CRITERIA**:
- âœ… Token budget enforced (8K for Mistral, 32K for Qwen3)
- âœ… Priority-based eviction: keep CRITICAL, evict BACKGROUND first
- âœ… Fast access (<1ms for cache hit)
- âœ… Automatic eviction when budget exceeded

**FAILING CRITERIA**:
- âŒ Cache grows beyond token budget
- âŒ Eviction removes CRITICAL items before BACKGROUND
- âŒ Cache access slow (>10ms)

**EDGE CASES**:
1. **All items CRITICAL priority** - evict oldest CRITICAL
2. **Cache empty, get() called** - return None gracefully
3. **Rapid updates** (100 items/sec) - should handle without lag
4. **Budget = 0** - should reject all additions

**Token Budget Allocation**:

**Mistral 7B (8K context)**:
```
Total: 8,192 tokens
â”œâ”€ System Prompt: 500 tokens (6%)
â”œâ”€ Task Description: 300 tokens (4%)
â”œâ”€ Current Context: 4,000 tokens (49%)
â”‚  â”œâ”€ Current module summary: 300 tokens
â”‚  â”œâ”€ Related functions: 1,200 tokens
â”‚  â”œâ”€ Dependencies: 800 tokens
â”‚  â”œâ”€ Recent errors: 400 tokens
â”‚  â””â”€ Test results: 1,300 tokens
â”œâ”€ Code Generation Space: 2,500 tokens (31%)
â””â”€ Buffer: 892 tokens (10%)
```

**Qwen3 8B (32K context)**:
```
Total: 32,768 tokens
â”œâ”€ System Prompt: 500 tokens (1.5%)
â”œâ”€ Task Description: 500 tokens (1.5%)
â”œâ”€ Current Context: 12,000 tokens (37%)
â”‚  â”œâ”€ Current module summary: 500 tokens
â”‚  â”œâ”€ Related modules (2 levels): 3,000 tokens
â”‚  â”œâ”€ Dependencies (detailed): 2,500 tokens
â”‚  â”œâ”€ Recent errors (last 5): 1,500 tokens
â”‚  â”œâ”€ Test results (full): 2,500 tokens
â”‚  â””â”€ Historical patterns: 2,000 tokens
â”œâ”€ Code Generation Space: 15,000 tokens (46%)
â””â”€ Buffer: 4,768 tokens (14%)
```

---

### 3.7 Offline Cache (Persistent Between Sessions)

**File**: `src/context/offline_cache.py` (â‰¤100 lines)
- [ ] Implement `OfflineCache` class
- [ ] Implement disk persistence with NetworkX graph serialization

**PASSING CRITERIA**:
- âœ… Graph persists to disk (JSON or pickle)
- âœ… load() restores full graph with all nodes
- âœ… search_semantic() finds similar nodes via embeddings
- âœ… PageRank ranking prioritizes important nodes

**FAILING CRITERIA**:
- âŒ Persistence fails silently
- âŒ load() corrupts graph or loses nodes
- âŒ Semantic search returns irrelevant nodes

**EDGE CASES**:
1. **File doesn't exist on load()** - initialize empty graph
2. **Corrupt graph file** - fallback to empty, log error
3. **Large graph** (10K nodes) - should compress or use graph DB
4. **No embeddings available** - fallback to keyword search

**Features**:
- **Semantic search**: Find nodes by meaning, not exact match
- **PageRank ranking**: Prioritize frequently-referenced nodes
- **Persistent storage**: Survives task completion
- **Historical patterns**: Learn from past solutions

---

### 3.8 DSPy-Based Summary Generation

**File**: `src/context/dspy_summarizers.py` (â‰¤100 lines)
- [ ] Implement `ModuleSummarizer` DSPy signature
- [ ] Implement `FunctionSummarizer` DSPy signature
- [ ] Implement `TaskSummarizer` DSPy signature

**PASSING CRITERIA**:
- âœ… Summaries have structured fields (purpose, dependencies, etc.)
- âœ… Type-safe outputs via DSPy signatures
- âœ… Consistent format across all summaries
- âœ… Summary generation completes in <5s

**FAILING CRITERIA**:
- âŒ Summaries missing required fields
- âŒ Inconsistent format between calls
- âŒ Generation times out (>30s)

**EDGE CASES**:
1. **Empty code input** - should generate "No code yet" summary
2. **Malformed code** (syntax errors) - should still summarize
3. **Very long code** (10K lines) - should truncate or sample
4. **LLM failure** - should retry or use fallback

**DSPy Signatures**:

```python
class ModuleSummarizer(dspy.Signature):
    """Generate structured summary of code module"""

    # Inputs
    module_name: str = dspy.InputField(desc="Name of the module")
    source_code: str = dspy.InputField(desc="Full source code")
    test_results: str = dspy.InputField(desc="Test execution results")
    iteration_history: str = dspy.InputField(desc="Refinement history")

    # Outputs
    purpose: str = dspy.OutputField(desc="One-line purpose of module")
    dependencies: list[str] = dspy.OutputField(desc="List of dependencies")
    functions: list[str] = dspy.OutputField(desc="List of function names")
    key_decisions: str = dspy.OutputField(desc="Important design decisions")
    summary: str = dspy.OutputField(desc="200-token comprehensive summary")

class FunctionSummarizer(dspy.Signature):
    """Generate structured summary of single function"""

    # Inputs
    function_name: str = dspy.InputField()
    source_code: str = dspy.InputField()
    dependencies: list[str] = dspy.InputField()

    # Outputs
    purpose: str = dspy.OutputField(desc="What function does")
    inputs: str = dspy.OutputField(desc="Input parameters and types")
    outputs: str = dspy.OutputField(desc="Return value and type")
    side_effects: str = dspy.OutputField(desc="External changes made")
    summary: str = dspy.OutputField(desc="100-token summary")

class TaskSummarizer(dspy.Signature):
    """Generate structured summary of entire task"""

    # Inputs
    task_id: str = dspy.InputField()
    requirements: str = dspy.InputField()
    plan: str = dspy.InputField()
    module_results: dict = dspy.InputField()

    # Outputs
    goal: str = dspy.OutputField(desc="High-level goal")
    status: str = dspy.OutputField(desc="Current status")
    modules: list[str] = dspy.OutputField(desc="List of modules")
    success_rate: float = dspy.OutputField(desc="Test success rate 0-1")
    summary: str = dspy.OutputField(desc="100-token task summary")
```

---

### 3.9 Event Sourcing Integration

**File**: `src/context/event_cache_integration.py` (â‰¤100 lines)
- [ ] Implement `EventSourcedCache` class
- [ ] Implement cache reconstruction from events

**PASSING CRITERIA**:
- âœ… rebuild_cache() reconstructs cache from event history
- âœ… Time-travel: rebuild at any past timestamp
- âœ… Cache invalidation via vector clocks
- âœ… Event replay is deterministic (same events = same cache)

**FAILING CRITERIA**:
- âŒ Rebuild produces different cache on second run
- âŒ Time-travel fails or produces wrong state
- âŒ Cache invalidation misses stale entries

**EDGE CASES**:
1. **Empty event history** - return empty cache
2. **Events out of order** - sort by sequence_number first
3. **Rebuild at future timestamp** - return latest state
4. **Events with missing data fields** - use defaults

**Key Methods**:

```python
class EventSourcedCache:
    async def rebuild_cache(
        self,
        task_id: str,
        until_timestamp: Optional[str] = None
    ) -> NetworkedCache:
        """Rebuild cache state from event history"""
        events = await event_store.get_events(
            task_id=task_id,
            until=until_timestamp
        )

        cache = NetworkedCache()
        for event in sorted(events, key=lambda e: e.sequence_number):
            cache = self.apply_event(event, cache)

        return cache

    def apply_event(self, event: Event, cache: NetworkedCache):
        """Apply single event to cache (event sourcing pattern)"""
        if event.event_type == EventType.MODULE_COMPLETE:
            # Add module node to cache
            node = CacheNode(
                id=f"module:{event.data['module_name']}",
                type="module",
                summary=event.data.get('summary', ''),
                vector_clock=event.vector_clock.copy(),
                # ...
            )
            cache.add_node(node)

        # ... handle other event types
        return cache

    def is_cache_stale(
        self,
        cache_entry: CacheNode,
        latest_events: List[Event]
    ) -> bool:
        """Check if cache entry is stale via vector clocks"""
        for event in latest_events:
            # If event has newer vector clock than cache, cache is stale
            if VectorClock.happens_before(
                cache_entry.vector_clock,
                event.vector_clock
            ):
                return True
        return False
```

---

### 3.10 Hierarchical Context Builder

**File**: `src/context/hierarchical_builder.py` (â‰¤100 lines)
- [ ] Implement `HierarchicalContextBuilder` class
- [ ] Implement multi-level summary assembly

**PASSING CRITERIA**:
- âœ… Build context in priority order (CRITICAL first)
- âœ… Respect token budget (stop at 80% to reserve generation space)
- âœ… Include explicit references between levels
- âœ… Gracefully handle missing nodes

**FAILING CRITERIA**:
- âŒ Context exceeds token budget
- âŒ Priority order not respected
- âŒ Missing references between levels

**EDGE CASES**:
1. **Token budget = 1000, but CRITICAL context = 1500** - truncate CRITICAL
2. **All nodes are LOW priority** - still include what fits
3. **Referenced node missing** - skip reference, continue
4. **Circular references** - detect and break cycle

**Context Priority Levels**:

1. **CRITICAL** (Always include):
   - Current task description
   - Current module summary
   - Immediate dependencies
   - Latest error/test failure

2. **HIGH** (Include if space):
   - Related function summaries
   - Dependency code snippets
   - Recent iteration history (last 2)

3. **MEDIUM** (Include if space):
   - Test code for current module
   - Related modules (same task)
   - Library usage patterns

4. **LOW** (Include if space):
   - Historical solutions (similar tasks)
   - Project-wide patterns
   - Full error trail

**Multi-Level Summary Example**:

```markdown
# Level 1: Task Summary (100 tokens)
Task: task-abc123
Status: Completed
Goal: Build email/phone validator module
Modules: [â†’ validators.py], [â†’ test_validators.py]
Success Rate: 100% (12/12 tests passed)

# Level 2: Module Summary (300 tokens)
## Module: validators.py [ID: mod-val-001]
Purpose: Email and phone validation functions
Dependencies: [â†’ pydantic], [â†’ phonenumbers], [â†’ re]
Functions:
  - [â†’ validate_email]: RFC 5322 email validation
  - [â†’ validate_phone]: International phone validation
Tests: [â†’ test_validators.py] (12/12 passed)
Full Code: [storage://task-abc123/validators.py]

# Level 3: Function Summary (150 tokens)
## Function: validate_email [ID: func-val-email-001]
Module: [â† validators.py]
Purpose: Validate email addresses using RFC 5322 regex
Input: email: str
Output: bool (True if valid)
Dependencies: [â†’ re module]
Test Coverage: [â†’ test_email_valid], [â†’ test_email_invalid]
```

**Note**: Arrows ([â†’], [â†]) are explicit references that enable graph traversal.

---

### ğŸ§ª TEST GATE 3 (Enhanced): Networked Cache Tests

**File**: `tests/test_networked_cache.py` (â‰¤100 lines)

```python
# Test 1: Node relationships
âœ… PASS: add_node() creates edges for dependencies and references
âœ… PASS: get_related_context() follows graph edges within depth=2
âŒ FAIL: Edges missing or graph traversal fails

# Test 2: Token budget respect
âœ… PASS: get_related_context(max_tokens=1000) returns â‰¤1000 tokens
âŒ FAIL: Returns >1000 tokens

# Test 3: Graph persistence
âœ… PASS: save() â†’ load() restores all nodes and edges
âŒ FAIL: Nodes or edges lost after load()

# Test 4: Circular dependency handling
âœ… PASS: Aâ†’Bâ†’Câ†’A detected, breaks cycle gracefully
âŒ FAIL: Infinite loop or crash
```

**File**: `tests/test_online_offline_cache.py` (â‰¤100 lines)

```python
# Test 1: Online cache eviction
âœ… PASS: Budget=1000 tokens, add 1500 â†’ evicts lowest priority first
âŒ FAIL: Doesn't evict or evicts wrong items

# Test 2: Offline cache semantic search
âœ… PASS: search_semantic("email validation") finds validate_email function
âŒ FAIL: Returns irrelevant nodes

# Test 3: Cache migration
âœ… PASS: Online cache â†’ Offline cache preserves all data
âŒ FAIL: Data lost during migration
```

**File**: `tests/test_dspy_summarizers.py` (â‰¤100 lines)

```python
# Test 1: Structured outputs
âœ… PASS: ModuleSummarizer returns all required fields (purpose, dependencies, etc.)
âŒ FAIL: Missing fields or wrong types

# Test 2: Consistent format
âœ… PASS: Two calls with same input â†’ identical output structure
âŒ FAIL: Inconsistent formats

# Test 3: Error handling
âœ… PASS: Empty code input â†’ generates valid summary (not error)
âŒ FAIL: Crashes or returns None
```

**File**: `tests/test_event_cache_integration.py` (â‰¤100 lines)

```python
# Test 1: Cache reconstruction
âœ… PASS: rebuild_cache() from 50 events produces correct cache state
âŒ FAIL: Cache state incorrect or incomplete

# Test 2: Time-travel
âœ… PASS: rebuild_cache(until="2024-01-15") returns cache as of that date
âŒ FAIL: Wrong state or includes future events

# Test 3: Vector clock invalidation
âœ… PASS: is_cache_stale() detects stale entry when new event arrived
âŒ FAIL: Doesn't detect staleness or false positive
```

**File**: `tests/test_hierarchical_builder.py` (â‰¤100 lines)

```python
# Test 1: Priority ordering
âœ… PASS: CRITICAL context included first, LOW last
âŒ FAIL: Wrong priority order

# Test 2: Token budget enforcement
âœ… PASS: Budget=8000, context stops at 6400 (80%)
âŒ FAIL: Exceeds 80% threshold

# Test 3: Explicit references
âœ… PASS: Context includes [â†’ ref] markers for related nodes
âŒ FAIL: Missing references or broken links
```

**Run Tests**:
```bash
pytest tests/test_networked_cache.py \
       tests/test_online_offline_cache.py \
       tests/test_dspy_summarizers.py \
       tests/test_event_cache_integration.py \
       tests/test_hierarchical_builder.py -v
```

â›” **STOP HERE IF ANY TEST FAILS** â›”

---

## Summary: What This Solves

**Problem**: Context compression wipes out event details, causing small LLMs to forget:
- **What** they were doing (task goal, current module)
- **Why** they were doing it (design decisions, iteration history)
- **How** they were doing it (approaches tried, test results)
- **3rd/4th degree relations** (dependencies of dependencies)

**Solution**: Networked cache with explicit references:
- **Online cache**: Fast, live context for current task (8K-32K tokens)
- **Offline cache**: Persistent historical knowledge (unlimited)
- **Graph structure**: Explicit references preserve relationships
- **DSPy summaries**: Structured, type-safe summaries with consistent format
- **Event sourcing**: Rebuild cache at any point, debug state evolution
- **Vector clocks**: Invalidate stale cache entries automatically

**Result**: Small LLMs (Mistral 7B, Qwen3 8B) can maintain context without forgetting critical information, even with aggressive compression (70% reduction while retaining 70% information vs 50% in flat summaries).

---

## Integration Notes

**Existing Code Compatibility**:
- âœ… Works with existing EventStore (Phase 1)
- âœ… Uses VectorClock for invalidation (Phase 1)
- âœ… Integrates with TaskOrchestrator priority system (Phase 2)
- âœ… Builds on existing WorkflowEvent types

**Migration Path**:
1. Implement networked cache alongside existing context management
2. Generate cache nodes from existing task states
3. Gradually replace simple compression with graph-based retrieval
4. Monitor effectiveness: compression ratio vs information retention

**Performance Considerations**:
- Graph traversal: O(N) where N = number of related nodes (typically <50)
- Semantic search: O(log K) where K = total nodes (using embeddings + FAISS)
- Cache rebuild: O(E) where E = number of events (typically <1000)
- All operations <100ms for typical task sizes

---

## Phase 4: FastAPI + WebSocket Layer ğŸŒ

### 4.1 Pydantic Models

**File**: `src/api/models/requests.py` (â‰¤100 lines)
- [ ] Define `TaskRequest`, `InterventionRequest` models

**PASSING CRITERIA**:
- âœ… TaskRequest.prompt is non-empty string
- âœ… TaskRequest.answers is list (can be empty)
- âœ… Pydantic validation rejects invalid data

**FAILING CRITERIA**:
- âŒ Accepts empty prompt ""
- âŒ Accepts answers as non-list
- âŒ Validation doesn't catch malformed data

**EDGE CASES**:
1. **prompt with 100K chars** - should handle or reject with clear error
2. **answers with 1000 items** - should handle
3. **Unicode/emoji in prompt** - should handle
4. **priority="CRITICAL"** (string) - should convert to TaskPriority enum
5. **priority="INVALID"** - should reject with error

**File**: `src/api/models/responses.py` (â‰¤100 lines)
- [ ] Define response models

**PASSING CRITERIA**:
- âœ… All required fields present
- âœ… Serialization to JSON works
- âœ… websocket_url format is valid (ws://host:port/ws/task_id)

**FAILING CRITERIA**:
- âŒ Missing required fields
- âŒ Non-serializable fields
- âŒ Invalid URL format

**EDGE CASES**:
1. **task_id with special chars** - URL encoding should handle
2. **Very long task_id** (1000 chars) - should handle or reject
3. **None values** - should serialize as null

---

### 4.2 WebSocket Manager

**File**: `src/api/websocket_manager.py` (â‰¤100 lines)
- [ ] Implement `WebSocketManager` class

**PASSING CRITERIA**:
- âœ… connect() accepts WebSocket and stores in dict
- âœ… Multiple clients can connect to same task_id
- âœ… broadcast_to_task() sends to all connected clients
- âœ… disconnect() removes client and cleans up empty lists
- âœ… Failed sends don't crash other broadcasts

**FAILING CRITERIA**:
- âŒ Only 1 client per task_id allowed
- âŒ broadcast fails if one client disconnected
- âŒ Memory leak (disconnected clients not removed)

**EDGE CASES**:
1. **Client connects, then immediately disconnects** - should handle
2. **broadcast to task_id with no clients** - should be no-op, not error
3. **Client connection fails during accept** - should handle gracefully
4. **100 clients for same task_id** - should broadcast to all efficiently
5. **Client receives message, then disconnects** - no error
6. **WebSocket send raises exception** - should catch and remove client
7. **Concurrent connect/disconnect** - should be thread-safe

**CRITICAL**: Must integrate with existing `StreamingWorkflowOrchestrator`!

---

### 4.3 Task Manager (FastAPI Integration)

**File**: `src/api/task_manager.py` (â‰¤100 lines)
- [ ] Implement `TaskManager` class

**PASSING CRITERIA**:
- âœ… start_task() creates asyncio.Task and stores in dict
- âœ… Callback cleanup when task completes
- âœ… cancel_task() cancels and awaits task
- âœ… get_task_status() queries event store (not in-memory state)
- âœ… shutdown_all() cancels all tasks

**FAILING CRITERIA**:
- âŒ start_task() blocks until task completes
- âŒ Completed tasks not removed (memory leak)
- âŒ cancel_task() doesn't await (leaves zombie tasks)
- âŒ shutdown_all() misses some tasks

**EDGE CASES**:
1. **Task raises exception** - callback should still fire
2. **cancel_task() on completed task** - should handle gracefully
3. **Double cancel** - should be idempotent
4. **get_task_status() for unknown task_id** - should return error, not crash
5. **start_task() with duplicate task_id** - should reject or overwrite

**CRITICAL INTEGRATION**: Must work with existing `StateManager` global dict!

---

### 4.4-4.6 Endpoints

**Files**: `src/api/endpoints/*.py` (â‰¤100 lines each)
- [ ] health.py, tasks.py, websocket.py

**PASSING CRITERIA**:
- âœ… POST /tasks returns 200 with task_id
- âœ… GET /tasks/{task_id} returns current status
- âœ… WebSocket /ws/{task_id} accepts connection
- âœ… WebSocket receives versioned messages
- âœ… GET /health returns 200 when healthy

**FAILING CRITERIA**:
- âŒ POST /tasks with invalid data returns 200 (should be 400)
- âŒ GET /tasks/nonexistent returns 200 (should be 404)
- âŒ WebSocket doesn't send version in messages
- âŒ Health check returns 200 when Ollama down (should be 503)

**EDGE CASES**:
1. **POST /tasks during high load** - should queue or reject gracefully
2. **GET /tasks/{id} for task that hasn't started** - should return status "created"
3. **WebSocket connect before task starts** - should wait or error clearly
4. **WebSocket receives out-of-order messages** - client should sort by sequence
5. **Health check during DSPy reconfiguration** - should handle

---

### 4.7 Versioned State Manager

**File**: `src/api/versioned_state.py` (â‰¤100 lines)
- [ ] Implement `VersionedState` class

**PASSING CRITERIA**:
- âœ… update() rejects version â‰¤ current_version
- âœ… update() is thread-safe (asyncio.Lock)
- âœ… get_current_version() returns latest version
- âœ… States indexed by version number

**FAILING CRITERIA**:
- âŒ Accepts stale update (v2 after v3)
- âŒ Race condition on concurrent updates
- âŒ Version collision (two states with same version)

**EDGE CASES**:
1. **Update v1, v3 (skip v2)** - should accept (versions can have gaps)
2. **Concurrent update of v2 from two sources** - only one should succeed
3. **Update with v=0** - should accept as initial
4. **get_current_version() with no updates** - should return 0
5. **Very large version number** (2^31) - should handle

### ğŸ§ª TEST GATE 4: FastAPI Tests

**File**: `tests/test_api_models.py` (â‰¤100 lines)

```python
# Test 1: Pydantic validation
âœ… PASS: TaskRequest(prompt="test") â†’ valid
âŒ FAIL: TaskRequest(prompt="") â†’ ValidationError not raised

# Test 2: Serialization
âœ… PASS: TaskResponse(...).dict() â†’ valid JSON dict
âŒ FAIL: Non-serializable fields cause error
```

**File**: `tests/test_websocket_manager.py` (â‰¤100 lines)

```python
# Test 1: Multiple clients
âœ… PASS: 3 clients connect to task-A â†’ all 3 receive broadcast
âŒ FAIL: Only 1 receives, or broadcast fails

# Test 2: Cleanup
âœ… PASS: Client disconnects â†’ removed from active_connections
âŒ FAIL: Client lingers (memory leak)

# Test 3: Failed send
âœ… PASS: 1 of 3 clients fails â†’ other 2 still receive message
âŒ FAIL: All broadcasts fail due to 1 bad client
```

**File**: `tests/test_task_manager.py` (â‰¤100 lines)

```python
# Test 1: Task creation
âœ… PASS: start_task() creates asyncio.Task
âœ… PASS: Task ID in active_tasks dict
âŒ FAIL: Task not tracked

# Test 2: Cancellation
âœ… PASS: cancel_task() cancels and removes from dict
âŒ FAIL: Task still running, or not removed

# Test 3: Callback cleanup
âœ… PASS: Task completes â†’ callback fires â†’ removed from dict
âŒ FAIL: Callback doesn't fire, task lingers
```

**File**: `tests/test_endpoints.py` (â‰¤100 lines)

```python
# Test 1: POST /tasks
âœ… PASS: Valid request â†’ 200, returns task_id
âŒ FAIL: Invalid request â†’ 200 (should be 400)

# Test 2: GET /tasks/{id}
âœ… PASS: Existing task â†’ 200, returns status
âœ… PASS: Nonexistent task â†’ 404
âŒ FAIL: 404 returns 200, or 200 returns 500

# Test 3: GET /health
âœ… PASS: DSPy configured, Ollama reachable â†’ 200
âŒ FAIL: Returns 200 when Ollama down
```

**File**: `tests/test_websocket_endpoint.py` (â‰¤100 lines)

```python
# Test 1: Connection
âœ… PASS: WS /ws/task-A connects successfully
âŒ FAIL: Connection refused or timeout

# Test 2: Message versioning
âœ… PASS: All messages include version and sequence fields
âŒ FAIL: Missing version or sequence

# Test 3: Stale update rejection
âœ… PASS: Client at v3 receives v2 message â†’ ignores
âŒ FAIL: Client processes stale message
```

**Run Tests**:
```bash
pytest tests/test_api_models.py tests/test_websocket_manager.py tests/test_task_manager.py tests/test_endpoints.py tests/test_websocket_endpoint.py -v
```

â›” **STOP HERE IF ANY TEST FAILS** â›”

---

## Phase 5: Hybrid FastAPI + FastMCP Main App ğŸ”—

### 5.1 Main FastAPI App

**File**: `src/api/main.py` (â‰¤100 lines)

**PASSING CRITERIA**:
- âœ… Lifespan startup configures DSPy successfully
- âœ… Orchestrator workers start (3 workers running)
- âœ… All routers mounted (/health, /tasks, /ws)
- âœ… FastMCP mounted at /mcp/
- âœ… app.state contains all managers
- âœ… Lifespan shutdown cancels all tasks

**FAILING CRITERIA**:
- âŒ DSPy configuration fails, app still starts
- âŒ Workers don't start
- âŒ FastMCP not accessible at /mcp/
- âŒ Shutdown doesn't clean up tasks

**EDGE CASES**:
1. **Ollama not running at startup** - should error clearly, not start
2. **Port 9876 already in use** - should error clearly
3. **Shutdown during active task** - should wait for cancellation
4. **Restart after crash** - should reinitialize cleanly

---

### 5.2 Modify Existing MCP Server

**File**: `src/mcp/fastmcp_server.py` (MODIFY)

**CHANGES**:
```python
# Add at end (line 151+):
def get_asgi_app():
    """Export FastMCP as ASGI app for mounting"""
    return mcp.get_asgi_app()
```

**PASSING CRITERIA**:
- âœ… get_asgi_app() returns valid ASGI app
- âœ… All existing tools still work via stdio
- âœ… Mounted app works via HTTP at /mcp/

**FAILING CRITERIA**:
- âŒ get_asgi_app() returns None or errors
- âŒ stdio mode broken
- âŒ HTTP mode broken

**EDGE CASES**:
1. **stdio and HTTP used simultaneously** - should work independently
2. **global state shared** - should be thread-safe

---

### 5.3 Update Server Runner

**File**: `src/mcp/server_runner.py` (MODIFY)

**CHANGES**:
- Add --fastapi flag
- If --fastapi: `subprocess.run(["uvicorn", "src.api.main:app", ...])`

**PASSING CRITERIA**:
- âœ… --fastapi starts hybrid server on port 9876
- âœ… --http still works (FastMCP HTTP only)
- âœ… Default (no flags) still works (stdio)

**FAILING CRITERIA**:
- âŒ --fastapi doesn't start server
- âŒ Breaking existing modes

### ğŸ§ª TEST GATE 5: Integration Tests

**File**: `tests/test_hybrid_integration.py` (â‰¤100 lines)

```python
# Test 1: FastAPI endpoints work
âœ… PASS: POST /tasks returns 200
âŒ FAIL: 404 or 500

# Test 2: FastMCP tools work
âœ… PASS: POST /mcp/tools/call â†’ start_coding_task works
âŒ FAIL: 404 or tool doesn't execute

# Test 3: Shared state
âœ… PASS: Task created via REST visible via MCP get_task_status
âŒ FAIL: State not shared

# Test 4: Lifespan
âœ… PASS: Startup configures DSPy, logs confirm
âœ… PASS: Shutdown cancels tasks, logs confirm
âŒ FAIL: Startup/shutdown errors
```

**File**: `tests/test_network_access.py` (â‰¤100 lines)

```python
# Test 1: 0.0.0.0 binding
âœ… PASS: Server binds to 0.0.0.0:9876
âœ… PASS: Accessible from 127.0.0.1
âŒ FAIL: Only localhost, or port conflict

# Test 2: WebSocket from network
âœ… PASS: WS connection from external IP works
âŒ FAIL: Connection refused

# Test 3: MCP endpoint
âœ… PASS: http://localhost:9876/mcp/ returns MCP info
âŒ FAIL: 404 or error
```

**Run Tests**:
```bash
pytest tests/test_hybrid_integration.py tests/test_network_access.py -v
```

â›” **STOP HERE IF ANY TEST FAILS** â›”

---

## Phase 6: Workflow Integration ğŸ”„

### 6.1 Modify Workflow Orchestrator

**File**: `src/orchestrator/workflow.py` (MODIFY)

**CRITICAL INTEGRATION NOTES**:
- Existing WorkflowOrchestrator (line 30-166)
- Existing StreamingWorkflowOrchestrator (in streaming.py)
- Must add event emission WITHOUT breaking existing functionality

**PASSING CRITERIA**:
- âœ… Existing workflow still works without WebSocket manager
- âœ… With WebSocket manager, events emitted at each stage
- âœ… Events stored in EventStore
- âœ… Backward compatible (Optional parameters)

**FAILING CRITERIA**:
- âŒ Workflow breaks if event_store=None
- âŒ Events not emitted
- âŒ EventStore not populated

**EDGE CASES**:
1. **websocket_manager=None** - should work (for backward compat)
2. **event_store=None** - should work (for backward compat)
3. **Both provided** - should emit events to both
4. **WebSocket broadcast fails** - should continue workflow
5. **EventStore append fails** - should log error, continue workflow

**INTEGRATION WITH EXISTING EVENT SYSTEM**:
- Existing `WorkflowEvent` in orchestrator/events.py
- New `Event` in events/event.py
- Must coexist! Perhaps emit both temporarily.

---

### 6.2 Modify State Manager

**File**: `src/orchestrator/state.py` (MODIFY)

**CHANGES**:
```python
# Line 23+ in TaskState.__init__:
self.asyncio_task: Optional[asyncio.Task] = None
self.websocket_connections: List[WebSocket] = []
self.code_version: int = 0
```

**PASSING CRITERIA**:
- âœ… New fields added
- âœ… Existing fields unchanged
- âœ… Backward compatible (old code still works)

**FAILING CRITERIA**:
- âŒ Breaking changes to existing fields
- âŒ Import errors

**CRITICAL**: StateManager uses dict, NOT thread-safe! Must add locks or use async-safe structure.

### ğŸ§ª TEST GATE 6: Workflow Integration Tests

**File**: `tests/test_workflow_events.py` (â‰¤100 lines)

```python
# Test 1: Event emission
âœ… PASS: Workflow emits TASK_STARTED, PLANNING_COMPLETE, MODULE_STARTED, etc.
âŒ FAIL: Events missing or out of order

# Test 2: Version increment
âœ… PASS: CODE_GENERATED events have version [1,2,3...]
âŒ FAIL: Version doesn't increment, or skips numbers

# Test 3: WebSocket broadcast
âœ… PASS: Connected client receives all events
âŒ FAIL: Client misses events

# Test 4: EventStore population
âœ… PASS: EventStore contains all workflow events after completion
âŒ FAIL: Events missing or corrupted
```

**File**: `tests/test_end_to_end.py` (â‰¤100 lines)

```python
# Test 1: Full workflow
âœ… PASS: POST /tasks â†’ workflow completes â†’ GET /tasks/{id} shows "completed"
âŒ FAIL: Workflow hangs, errors, or status wrong

# Test 2: WebSocket receives all events
âœ… PASS: Client receives [TASK_STARTED ... TASK_COMPLETE] in order
âŒ FAIL: Events out of order, or missing

# Test 3: Version monotonicity
âœ… PASS: All events have increasing version numbers
âŒ FAIL: Version decreases or repeats

# Test 4: Event history
âœ… PASS: EventStore.get_events(task_id) returns complete history
âŒ FAIL: History incomplete

# Test 5: Context manager used
âœ… PASS: DSPy calls use context from ContextManager
âŒ FAIL: Context not used, or exceeds model limits
```

**Run Tests**:
```bash
pytest tests/test_workflow_events.py tests/test_end_to_end.py -v --tb=long
```

â›” **STOP HERE IF ANY TEST FAILS** â›”

---

## Summary: Critical Edge Cases Across All Phases

### Thread Safety Issues (HIGH PRIORITY):
1. StateManager.tasks - concurrent access
2. InterventionManager.interventions - concurrent access
3. EventStore.sequence_counter - must use lock
4. AsyncLogger.logs - list append not atomic
5. WebSocketManager.active_connections - dict modification

### Data Validation Issues:
6. task_id - never validated (could be empty, None, malicious)
7. prompt - could be empty string ""
8. code_version - could be negative
9. event.data - could contain non-serializable objects
10. module_spec.dependencies - could be malformed

### Integration Gotchas:
11. Two event systems (WorkflowEvent vs Event) - must coexist
12. StreamingWorkflowOrchestrator already exists - must integrate
13. Global state managers - must be made async-safe
14. TestRunner subprocess.run - blocks event loop, not truly async
15. AsyncLogger file writes - not atomic, race condition

### Performance Edge Cases:
16. 1M events in EventStore - projection should be <5s
17. 100 concurrent WebSocket clients - broadcast should be <100ms
18. Queue with 100 tasks - submit should block, not error
19. Very long context (50K tokens) - should route to larger model
20. 1000 task_ids in StateManager - lookup should be O(1)

### Error Handling:
21. Ollama not running - should fail fast with clear error
22. Disk full during event persist - should handle gracefully
23. WebSocket client disconnects mid-broadcast - should continue
24. DSPy returns malformed result - should validate and retry
25. Test timeout during high load - should cancel gracefully

---

## Final Pre-Implementation Checklist

Before starting Phase 1:
- [ ] Read this entire document
- [ ] Understand all edge cases
- [ ] Understand integration with existing code (events.py, streaming.py, state.py)
- [ ] Have pytest installed and working
- [ ] Have uvicorn installed (pip install uvicorn[standard])
- [ ] Ollama running (http://localhost:11434)
- [ ] Test existing workflow works (python -m src.mcp.server_runner)

**Current Status**: â¸ï¸ Awaiting user approval and write access to `src/`
